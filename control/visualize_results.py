#!/usr/bin/env python3
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
–°–æ–∑–¥–∞–µ—Ç –Ω–∞–≥–ª—è–¥–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏.
"""
import json
import sys
import os
import re
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
matplotlib.use('Agg')  # –î–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ GUI

# –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞ –¥–ª—è —Ç–∏–ø–æ–≤ —Ö—Ä–∞–Ω–∏–ª–∏—â
STORAGE_COLORS = {
    'local': '#1f77b4',  # –°–∏–Ω–∏–π –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    'iscsi': '#ff7f0e',  # –û—Ä–∞–Ω–∂–µ–≤—ã–π –¥–ª—è iSCSI
    'default': '#2ca02c'  # –ó–µ–ª–µ–Ω—ã–π –¥–ª—è –Ω–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤
}

def get_storage_type(label):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∏–ø —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∏–∑ –º–µ—Ç–∫–∏"""
    label_lower = label.lower()
    if 'local' in label_lower:
        return 'local'
    elif 'iscsi' in label_lower:
        return 'iscsi'
    else:
        return 'default'

def get_color_for_storage(storage_type):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–≤–µ—Ç –¥–ª—è —Ç–∏–ø–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    return STORAGE_COLORS.get(storage_type, STORAGE_COLORS['default'])

def load_aggregated_data(json_file):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        with open(json_file, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {json_file}: {str(e)}")
        return None

def add_value_labels(ax, bars, values):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞/–≤–Ω—É—Ç—Ä–∏ —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    max_height = max(bar.get_height() for bar in bars if bar.get_height() > 0)
    
    for i, (bar, value) in enumerate(zip(bars, values)):
        height = bar.get_height()
        if height == 0:
            continue
            
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —Ç–µ–∫—Å—Ç–∞
        if height > max_height * 0.1:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π —Å—Ç–æ–ª–±–µ—Ü
            if height > 10:  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π —Å—Ç–æ–ª–±–µ—Ü - —Ç–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏
                text_y = height * 0.5
                va = 'center'
                color = 'white'
                fontsize = max(8, 10 - len(bars))
            else:  # –°—Ä–µ–¥–Ω–∏–π —Å—Ç–æ–ª–±–µ—Ü - —Ç–µ–∫—Å—Ç –Ω–µ–º–Ω–æ–≥–æ –≤—ã—à–µ
                text_y = height + (max_height * 0.05)
                va = 'bottom'
                color = 'black'
                fontsize = 9
        else:  # –ú–∞–ª–µ–Ω—å–∫–∏–π —Å—Ç–æ–ª–±–µ—Ü - —Ç–µ–∫—Å—Ç —Å–±–æ–∫—É
            text_y = height + (max_height * 0.05)
            va = 'bottom'
            color = 'black'
            fontsize = 8
        
        ax.text(bar.get_x() + bar.get_width()/2., text_y,
               f'{value:.1f}',
               ha='center', va=va, fontsize=fontsize, color=color,
               bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=0.5) if height < max_height * 0.1 else None)

def extract_pgbench_data(data):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ pgbench –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
    if 'pgbench' in data and isinstance(data['pgbench'], dict):
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π JSON —Ñ–æ—Ä–º–∞—Ç
        pg_data = data['pgbench']
        if 'TPS_mean' in pg_data:
            return {
                'TPS_mean': pg_data['TPS_mean'],
                'TPS_stdev': pg_data.get('TPS_stdev', 0),
                'Latency_Avg_mean': pg_data['Latency_Avg_mean'],
                'Latency_Avg_stdev': pg_data.get('Latency_Avg_stdev', 0),
                'samples': pg_data.get('samples', 1)
            }
        elif 'TPS' in pg_data:
            return {
                'TPS_mean': pg_data['TPS'],
                'TPS_stdev': pg_data.get('TPS_stdev', 0),
                'Latency_Avg_mean': pg_data['Latency_Avg'],
                'Latency_Avg_stdev': pg_data.get('Latency_Avg_stdev', 0),
                'samples': pg_data.get('samples', 1)
            }
    
    # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    if 'pgbench_section' in data:
        pgbench_text = data['pgbench_section']
        tps_match = re.search(r'TPS\s*(?:\(Transactions Per Second\))?:\s*([\d.]+)', pgbench_text)
        lat_match = re.search(r'–°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞:\s*([\d.]+)\s*ms', pgbench_text)
        
        if tps_match and lat_match:
            return {
                'TPS_mean': float(tps_match.group(1)),
                'TPS_stdev': 0.0,
                'Latency_Avg_mean': float(lat_match.group(1)),
                'Latency_Avg_stdev': 0.0,
                'samples': 1
            }
    
    # –ü–æ–∏—Å–∫ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö fio, –≥–¥–µ pgbench –º–æ–≥ –±—ã—Ç—å –∑–∞–ø—É—â–µ–Ω –æ—Ç–¥–µ–ª—å–Ω–æ
    if 'fio' in data and any('pgbench' in test_name.lower() for test_name in data['fio'].keys()):
        print(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã pgbench –≤ —Ä–∞–∑–¥–µ–ª–µ FIO –¥–ª—è {data.get('label', 'unknown')}")
    
    return None

def validate_data_for_visualization(datasets):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–æ–ª–µ–π –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å"""
    valid_datasets = {}
    test_types_found = set()
    
    for label, data in datasets.items():
        if not data or 'fio' not in data:
            print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç {label}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ FIO")
            continue
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–∏–ø—ã —Ç–µ—Å—Ç–æ–≤
        test_types = list(data['fio'].keys())
        test_types_found.update(test_types)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–æ–ª–µ–π –≤ –∫–∞–∂–¥–æ–º —Ç–µ—Å—Ç–µ
        valid_tests = {}
        for test_name, metrics in data['fio'].items():
            required_fields = ['IOPS_mean', 'Bandwidth_mean', 'Latency_mean']
            if all(field in metrics for field in required_fields):
                valid_tests[test_name] = metrics
            else:
                print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω —Ç–µ—Å—Ç '{test_name}' –≤ {label}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è")
        
        if not valid_tests:
            print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç {label}: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤")
            continue
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö —Å —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º–∏ —Ç–µ—Å—Ç–∞–º–∏
        valid_data = data.copy()
        valid_data['fio'] = valid_tests
        valid_datasets[label] = valid_data
    
    if not valid_datasets:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        return None, None
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(valid_datasets)} –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã —Ç–µ—Å—Ç—ã: {', '.join(sorted(test_types_found))}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    standard_tests = [
        "Sequential Write",
        "Sequential Read",
        "Random Write",
        "Random Read",
        "Mixed RW (Write)",
        "Mixed RW (Read)"
    ]
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–µ—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
    filtered_tests = [test for test in standard_tests if test in test_types_found]
    
    if not filtered_tests:
        print("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        filtered_tests = sorted(test_types_found)[:6]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 6 —Ç–µ—Å—Ç–æ–≤
    
    return valid_datasets, filtered_tests

def plot_fio_comparison(datasets, filtered_tests, output_dir):
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è FIO —Ç–µ—Å—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    if not filtered_tests or not datasets:
        print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ FIO")
        return
    
    # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏
    metrics = ['IOPS', 'Bandwidth', 'Latency']
    metric_titles = {
        'IOPS': '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ IOPS –º–µ–∂–¥—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏',
        'Bandwidth': '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏',
        'Latency': '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–µ–∫ –º–µ–∂–¥—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏'
    }
    metric_labels = {
        'IOPS': 'IOPS (—Ç—ã—Å—è—á–∏)',
        'Bandwidth': 'Bandwidth (MiB/s)',
        'Latency': 'Latency (ms)'
    }
    
    x = range(len(filtered_tests))
    width = 0.8 / len(datasets)
    
    for metric in metrics:
        fig, ax = plt.subplots(figsize=(14, 8))
        
        # –î–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–¥–∞—Ç–∞—Å–µ—Ç–∞)
        for idx, (label, data) in enumerate(datasets.items()):
            values = []
            errors = []
            
            for test in filtered_tests:
                if test in data['fio']:
                    values.append(data['fio'][test][f'{metric}_mean'])
                    errors.append(data['fio'][test][f'{metric}_stdev'])
                else:
                    values.append(0)
                    errors.append(0)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            storage_type = get_storage_type(label)
            color = get_color_for_storage(storage_type)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —Å—Ç–æ–ª–±—Ü–æ–≤
            offset = width * idx - width * (len(datasets) - 1) / 2
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã
            bars = ax.bar([i + offset for i in x], values, width,
                          yerr=errors, capsize=5, color=color, alpha=0.8,
                          label=f"{storage_type.upper()} ({data['num_vms']} VM)")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞/–≤–Ω—É—Ç—Ä–∏ —Å—Ç–æ–ª–±—Ü–æ–≤
            add_value_labels(ax, bars, values)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥—Ä–∞—Ñ–∏–∫–∞
        ax.set_xlabel('–¢–∏–ø —Ç–µ—Å—Ç–∞', fontsize=12)
        ax.set_ylabel(metric_labels[metric], fontsize=12)
        ax.set_title(metric_titles[metric], fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels([t.replace(' ', '\n') for t in filtered_tests], rotation=15, ha='center')
        ax.legend(title='–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è', loc='upper right')
        ax.grid(axis='y', alpha=0.3)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ç—Å—Ç—É–ø—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        plt.subplots_adjust(top=0.85)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.savefig(os.path.join(output_dir, f'fio_{metric.lower()}_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ FIO —Å–æ–∑–¥–∞–Ω—ã")

def plot_pgbench_comparison(datasets, output_dir):
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è pgbench —Ç–µ—Å—Ç–æ–≤"""
    pgbench_data = {}
    
    for label, data in datasets.items():
        pg_data = extract_pgbench_data(data)
        if pg_data:
            pgbench_data[label] = pg_data
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ pgbench –¥–ª—è {label}: TPS={pg_data['TPS_mean']:.1f}, Latency={pg_data['Latency_Avg_mean']:.3f}ms")
        else:
            print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ pgbench –¥–ª—è {label}")
    
    if not pgbench_data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ pgbench –Ω–∏ –≤ –æ–¥–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ")
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª-–∑–∞–≥–ª—É—à–∫—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        with open(os.path.join(output_dir, 'pgbench_comparison_missing_data.txt'), 'w') as f:
            f.write("–î–∞–Ω–Ω—ã–µ pgbench –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏–ª–∏ –Ω–µ –±—ã–ª–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã\n")
            f.write("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\n")
            for label, data in datasets.items():
                f.write(f"\n=== {label} ===\n")
                f.write(json.dumps(data, indent=2, ensure_ascii=False))
        return
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ pgbench –¥–ª—è {len(pgbench_data)} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π")
    
    # –°–æ–∑–¥–∞–µ–º –¥–≤–∞ –≥—Ä–∞—Ñ–∏–∫–∞ –≤ –æ–¥–Ω–æ–π —Ñ–∏–≥—É—Ä–µ
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # –ì—Ä–∞—Ñ–∏–∫ TPS
    x = range(len(pgbench_data))
    width = 0.6
    
    labels = list(pgbench_data.keys())
    tps_values = [data['TPS_mean'] for data in pgbench_data.values()]
    tps_errors = [data['TPS_stdev'] for data in pgbench_data.values()]
    
    storage_types = [get_storage_type(label) for label in labels]
    colors = [get_color_for_storage(st) for st in storage_types]
    
    for i, (label, value, error, color) in enumerate(zip(labels, tps_values, tps_errors, colors)):
        bar = ax1.bar(i, value, width, yerr=error, capsize=10, 
                     color=color, alpha=0.8, label=label)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ —Å—Ç–æ–ª–±–µ—Ü
        height = bar[0].get_height()
        ax1.text(bar[0].get_x() + bar[0].get_width()/2., height + (height * 0.05),
                f'{height:.0f}',
                ha='center', va='bottom', fontsize=9)
    
    ax1.set_xlabel('–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è', fontsize=12)
    ax1.set_ylabel('TPS (—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ —Å–µ–∫—É–Ω–¥—É)', fontsize=12)
    ax1.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ PostgreSQL (pgbench)', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels([label.replace('_','-') for label in labels], rotation=15, ha='center')
    ax1.grid(axis='y', alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ –∑–∞–¥–µ—Ä–∂–∫–∏
    lat_values = [data['Latency_Avg_mean'] for data in pgbench_data.values()]
    lat_errors = [data['Latency_Avg_stdev'] for data in pgbench_data.values()]
    
    for i, (label, value, error, color) in enumerate(zip(labels, lat_values, lat_errors, colors)):
        bar = ax2.bar(i, value, width, yerr=error, capsize=10,
                     color=color, alpha=0.8, label=label)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ —Å—Ç–æ–ª–±–µ—Ü
        height = bar[0].get_height()
        ax2.text(bar[0].get_x() + bar[0].get_width()/2., height + (height * 0.05),
                f'{height:.2f}',
                ha='center', va='bottom', fontsize=9)
    
    ax2.set_xlabel('–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è', fontsize=12)
    ax2.set_ylabel('–°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞ (ms)', fontsize=12)
    ax2.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–µ–∫ PostgreSQL (pgbench)', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels([label.replace('_','-') for label in labels], rotation=15, ha='center')
    ax2.grid(axis='y', alpha=0.3)
    
    # –°–æ–∑–¥–∞–µ–º –ª–µ–≥–µ–Ω–¥—É —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â
    unique_types = {}
    for label, st in zip(labels, storage_types):
        vm_match = re.search(r'_(\d+)vms_', label)
        vm_count = vm_match.group(1) if vm_match else "?"
        unique_types[f"{st.upper()} ({vm_count} VM)"] = get_color_for_storage(st)
    
    legend_elements = [plt.Rectangle((0,0),1,1, color=color, alpha=0.8) 
                      for color in unique_types.values()]
    ax1.legend(legend_elements, unique_types.keys(), title='–¢–∏–ø —Ö—Ä–∞–Ω–∏–ª–∏—â–∞')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'pgbench_comparison.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ pgbench —Å–æ–∑–¥–∞–Ω—ã")

def plot_scalability_analysis(datasets, output_dir):
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –í–ú –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    scalability_data = {}
    
    for label, data in datasets.items():
        storage_type = get_storage_type(label)
        vm_count = data.get('num_vms', 1)
        
        if storage_type not in scalability_data:
            scalability_data[storage_type] = {}
        
        # –°–æ–±–∏—Ä–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏
        if 'Random Read' in data['fio'] and 'Random Write' in data['fio']:
            scalability_data[storage_type][vm_count] = {
                'read_iops': data['fio']['Random Read']['IOPS_mean'],
                'write_iops': data['fio']['Random Write']['IOPS_mean']
            }
    
    if len(scalability_data) < 2 or any(len(vm_data) < 2 for vm_data in scalability_data.values()):
        print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏")
        return
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # –ì—Ä–∞—Ñ–∏–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ —á—Ç–µ–Ω–∏—è
    x_positions = range(len(next(iter(scalability_data.values()))))
    bar_width = 0.8 / len(scalability_data)
    
    for idx, (storage_type, vm_data) in enumerate(scalability_data.items()):
        vm_counts = sorted(vm_data.keys())
        read_iops = [vm_data[vm]['read_iops'] for vm in vm_counts]
        color = get_color_for_storage(storage_type)
        
        offset = bar_width * idx - bar_width * (len(scalability_data) - 1) / 2
        bars = ax1.bar([x + offset for x in x_positions], read_iops, bar_width,
                      color=color, alpha=0.8, label=storage_type.upper())
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + (height * 0.05),
                    f'{height:.0f}',
                    ha='center', va='bottom', fontsize=9)
    
    ax1.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –í–ú', fontsize=12)
    ax1.set_ylabel('Random Read IOPS', fontsize=12)
    ax1.set_title('–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å: Random Read', fontsize=14, fontweight='bold')
    ax1.set_xticks(x_positions)
    ax1.set_xticklabels(sorted(next(iter(scalability_data.values())).keys()))
    ax1.legend(title='–¢–∏–ø —Ö—Ä–∞–Ω–∏–ª–∏—â–∞')
    ax1.grid(axis='y', alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏
    for idx, (storage_type, vm_data) in enumerate(scalability_data.items()):
        vm_counts = sorted(vm_data.keys())
        write_iops = [vm_data[vm]['write_iops'] for vm in vm_counts]
        color = get_color_for_storage(storage_type)
        
        offset = bar_width * idx - bar_width * (len(scalability_data) - 1) / 2
        bars = ax2.bar([x + offset for x in x_positions], write_iops, bar_width,
                      color=color, alpha=0.8, label=storage_type.upper())
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + (height * 0.05),
                    f'{height:.0f}',
                    ha='center', va='bottom', fontsize=9)
    
    ax2.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –í–ú', fontsize=12)
    ax2.set_ylabel('Random Write IOPS', fontsize=12)
    ax2.set_title('–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å: Random Write', fontsize=14, fontweight='bold')
    ax2.set_xticks(x_positions)
    ax2.set_xticklabels(sorted(next(iter(scalability_data.values())).keys()))
    ax2.legend(title='–¢–∏–ø —Ö—Ä–∞–Ω–∏–ª–∏—â–∞')
    ax2.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'scalability_analysis.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω")

def debug_dataset_structure(datasets, output_dir):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
    debug_file = os.path.join(output_dir, 'data_structure_debug.txt')
    with open(debug_file, 'w') as f:
        f.write("="*80 + "\n")
        f.write("–°–¢–†–£–ö–¢–£–†–ê –î–ê–ù–ù–´–• –î–õ–Ø –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò\n")
        f.write("="*80 + "\n\n")
        
        for label, data in datasets.items():
            f.write(f"=== {label} ===\n")
            f.write(f"–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {type(data)}\n")
            
            if isinstance(data, dict):
                f.write("–ö–ª—é—á–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è:\n")
                for key in data.keys():
                    f.write(f"  - {key}\n")
                
                # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ FIO –¥–∞–Ω–Ω—ã—Ö
                if 'fio' in data and isinstance(data['fio'], dict):
                    f.write("\nFIO —Ç–µ—Å—Ç—ã:\n")
                    for test_name, metrics in data['fio'].items():
                        f.write(f"  - {test_name}: {', '.join(metrics.keys())}\n")
                        f.write(f"    –ó–Ω–∞—á–µ–Ω–∏—è: {json.dumps(metrics, indent=6)}\n")
                
                # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ pgbench –¥–∞–Ω–Ω—ã—Ö
                if 'pgbench' in data:
                    f.write("\npgbench –¥–∞–Ω–Ω—ã–µ:\n")
                    f.write(f"  {json.dumps(data['pgbench'], indent=4)}\n")
                elif 'pgbench_section' in data:
                    f.write("\npgbench_section (—Ç–µ–∫—Å—Ç):\n")
                    f.write(f"  {data['pgbench_section'][:200]}...\n")
            
            f.write("\n" + "="*80 + "\n")
    
    print(f"üîç –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏: {debug_file}")

def find_aggregated_reports(input_paths):
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ñ–∞–π–ª—ã —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø—É—Ç—è—Ö"""
    report_files = []
    
    for path in input_paths:
        path_obj = Path(path)
        
        # –ï—Å–ª–∏ —ç—Ç–æ —Ñ–∞–π–ª
        if path_obj.is_file():
            if path_obj.name == 'aggregated_report.json':
                report_files.append(str(path_obj))
            continue
        
        # –ï—Å–ª–∏ —ç—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –∏—â–µ–º –≤ –Ω–µ–π –∏ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
        if path_obj.is_dir():
            for file in path_obj.rglob('aggregated_report.json'):
                report_files.append(str(file))
            continue
    
    if not report_files:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        print(f"üîç –ü–æ–∏—Å–∫ –ø—Ä–æ–≤–æ–¥–∏–ª—Å—è –≤: {', '.join(input_paths)}")
        print("üîç –ò—Å–∫–∞–ª–∏—Å—å —Ñ–∞–π–ª—ã: aggregated_report.json")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        print("\nüìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:")
        for path in input_paths:
            path_obj = Path(path)
            if path_obj.is_dir():
                print(f"\n{path}:")
                for item in path_obj.rglob('*'):
                    if item.is_file():
                        print(f"  ‚Ä¢ {item.relative_to(path_obj)}")
    
    return report_files

def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python3 visualize_results.py <–ø–∞–ø–∫–∞1> [<–ø–∞–ø–∫–∞2> ...] –∏–ª–∏ <json_—Ñ–∞–π–ª1> [<json_—Ñ–∞–π–ª2> ...]")
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python3 visualize_results.py results/*/")
        print("  python3 visualize_results.py results/20251218_1619_local_1vms_2iter/ results/20251218_1722_iscsi_1vms_2iter/")
        print("  python3 visualize_results.py results/*/aggregated_report.json")
        sys.exit(1)
    
    # –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª—ã —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    report_files = find_aggregated_reports(sys.argv[1:])
    
    if not report_files:
        sys.exit(1)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    datasets = {}
    for json_path in report_files:
        data = load_aggregated_data(json_path)
        if data:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∫—É –∏–∑ –ø—É—Ç–∏
            parent_dir = Path(json_path).parent
            label = parent_dir.name
            datasets[label] = data
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω: {json_path} -> {label}")
    
    if not datasets:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        sys.exit(1)
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    valid_datasets, filtered_tests = validate_data_for_visualization(datasets)
    
    if not valid_datasets or not filtered_tests:
        print("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        sys.exit(1)
    
    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö
    debug_dataset_structure(valid_datasets, "visualization_output")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    output_dir = "visualization_output"
    os.makedirs(output_dir, exist_ok=True)
    print(f"\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤: {output_dir}/")
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
    plot_fio_comparison(valid_datasets, filtered_tests, output_dir)
    plot_pgbench_comparison(valid_datasets, output_dir)
    plot_scalability_analysis(valid_datasets, output_dir)
    
    print(f"\n‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìÅ –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}/")
    print("\n–°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    for file in sorted(os.listdir(output_dir)):
        if file.endswith('.png'):
            print(f"  ‚Ä¢ {file}")

if __name__ == "__main__":
    main()