#!/usr/bin/env python3
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –¥–∞–Ω–Ω—ã—Ö
"""
import os
import re
import json
import sys
from pathlib import Path
from statistics import mean, stdev
from datetime import datetime

def validate_fio_data(test_name, iops, bandwidth, latency):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫—É—é –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö fio"""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è IOPS –∏ Bandwidth –¥–ª—è 4k –±–ª–æ–∫–∞
    expected_bandwidth = iops * 4  # 4k –±–ª–æ–∫ = 4 KiB
    
    # –î–æ–ø—É—Å—Ç–∏–º–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ 20%
    min_allowed = expected_bandwidth * 0.8
    max_allowed = expected_bandwidth * 1.2
    
    # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π –∑–∞–ø–∏—Å–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω—å—à–µ –∏–∑-–∑–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è)
    write_exceptions = ["Write", "RW (Write)"]
    if any(exc in test_name for exc in write_exceptions):
        min_allowed = expected_bandwidth * 0.5
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —è–≤–Ω–æ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    if (iops > 100000 or bandwidth > 10000 or 
        (bandwidth > 0 and iops > 0 and not (min_allowed <= bandwidth <= max_allowed))):
        print(f"‚ö†Ô∏è –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è '{test_name}': IOPS={iops:.1f}, Bandwidth={bandwidth:.1f}")
        print(f"   –û–∂–∏–¥–∞–µ–º—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω bandwidth: {min_allowed:.1f}-{max_allowed:.1f}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–¥–µ—Ä–∂–µ–∫
    max_allowed_latency = {
        "Sequential Read": 50,
        "Sequential Write": 100,
        "Random Read": 100,
        "Random Write": 200,
        "Mixed RW": 200
    }
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ç–µ—Å—Ç–∞
    test_type = "Mixed RW" if "Mixed RW" in test_name else test_name.split()[0]
    max_latency = max_allowed_latency.get(test_type, 300)  # –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    if latency > max_latency:
        print(f"‚ö†Ô∏è –ê–Ω–æ–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è '{test_name}': {latency:.2f}ms (–º–∞–∫—Å. –¥–æ–ø—É—Å—Ç–∏–º–∞—è: {max_latency}ms)")
        return False
    
    return True

def parse_results_sheet(file_path):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –¥–∞–Ω–Ω—ã—Ö"""
    try:
        with open(file_path, 'r') as f:
            content = f.read()
        
        results = {'fio': {}, 'pgbench': {}, 'pgbench_section': ''}
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ fio
        fio_pattern = r'(\d+)\s+(.+?)\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)'
        for match in re.finditer(fio_pattern, content):
            test_num = int(match.group(1))
            test_name = match.group(2).strip()
            iops = float(match.group(3))
            bandwidth = float(match.group(4))
            latency = float(match.group(5))
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            if validate_fio_data(test_name, iops, bandwidth, latency):
                # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è —Ç–µ—Å—Ç–æ–≤ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –Ω–æ–º–µ—Ä–∞–º–∏
                unique_key = test_name
                if "Mixed RW" in test_name:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É —Å—Ç—Ä–æ–∫–∏
                    if "Read" in test_name or "–ß—Ç–µ–Ω–∏–µ" in test_name:
                        unique_key = "Mixed RW (Read)"
                    elif "Write" in test_name or "–ó–∞–ø–∏—Å—å" in test_name:
                        unique_key = "Mixed RW (Write)"
                
                results['fio'][unique_key] = {
                    'IOPS': iops,
                    'Bandwidth': bandwidth,
                    'Latency': latency
                }
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ pgbench
        pgbench_match = re.search(
            r'TPS\s*\(Transactions Per Second\):\s*([\d.]+).*?–°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞:\s*([\d.]+).*?–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π:\s*(\d+)',
            content, re.DOTALL
        )
        if pgbench_match:
            results['pgbench'] = {
                'TPS': float(pgbench_match.group(1)),
                'Latency_Avg': float(pgbench_match.group(2)),
                'Transactions': int(pgbench_match.group(3)),
                'samples': 1
            }
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å–µ–∫—Ü–∏—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            pgbench_section_match = re.search(r'===+–†–µ–∑—É–ª—å—Ç–∞—Ç—ã pgbench.*?(===+|$)', content, re.DOTALL)
            if pgbench_section_match:
                results['pgbench_section'] = pgbench_section_match.group(0)
        
        return results
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {file_path}: {str(e)}")
        return None

def get_vm_count_from_path(file_path):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –í–ú –∏–∑ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    path_str = str(file_path)
    match = re.search(r'_(\d+)vms_', path_str)
    if match:
        return int(match.group(1))
    return 1  # –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

def aggregate_results(results_dir):
    """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
    results_dir = Path(results_dir)
    all_result_files = list(results_dir.rglob('results_sheet_*.txt'))
    
    if not all_result_files:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        print(f"üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {results_dir}:")
        for item in results_dir.rglob('*'):
            if item.is_file():
                print(f"  ‚Ä¢ {item.relative_to(results_dir)}")
        return None
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(all_result_files)} —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    iterations_data = {}
    
    for file in all_result_files:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        iter_match = re.search(r'iter(\d+)', file.name)
        if not iter_match:
            continue
        
        iter_num = int(iter_match.group(1))
        parsed = parse_results_sheet(file)
        if parsed:
            if iter_num not in iterations_data:
                iterations_data[iter_num] = []
            iterations_data[iter_num].append(parsed)
    
    if not iterations_data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        return None
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –í–ú –∏–∑ –ø—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    vm_count = get_vm_count_from_path(results_dir)
    print(f"‚ÑπÔ∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –í–ú: {vm_count}")
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è FIO
    aggregated = {'fio': {}, 'pgbench': {}, 'iterations': sorted(iterations_data.keys()), 'num_vms': vm_count}
    all_fio_tests = set()
    
    for iter_results in iterations_data.values():
        for vm_result in iter_results:
            all_fio_tests.update(vm_result['fio'].keys())
    
    for test_name in sorted(all_fio_tests):
        metrics = {'IOPS': [], 'Bandwidth': [], 'Latency': []}
        for iter_results in iterations_data.values():
            for vm_result in iter_results:
                if test_name in vm_result['fio']:
                    metrics['IOPS'].append(vm_result['fio'][test_name]['IOPS'])
                    metrics['Bandwidth'].append(vm_result['fio'][test_name]['Bandwidth'])
                    metrics['Latency'].append(vm_result['fio'][test_name]['Latency'])
        
        if metrics['IOPS']:  # –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
            samples = len(metrics['IOPS'])
            aggregated['fio'][test_name] = {
                'IOPS_mean': mean(metrics['IOPS']),
                'IOPS_stdev': stdev(metrics['IOPS']) if samples > 1 else 0,
                'Bandwidth_mean': mean(metrics['Bandwidth']),
                'Bandwidth_stdev': stdev(metrics['Bandwidth']) if samples > 1 else 0,
                'Latency_mean': mean(metrics['Latency']),
                'Latency_stdev': stdev(metrics['Latency']) if samples > 1 else 0,
                'samples': samples
            }
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è pgbench
    pgbench_metrics = {'TPS': [], 'Latency_Avg': [], 'Transactions': []}
    pgbench_found = False
    
    for iter_results in iterations_data.values():
        for vm_result in iter_results:
            if 'pgbench' in vm_result and vm_result['pgbench']:
                pgbench_found = True
                pgbench_metrics['TPS'].append(vm_result['pgbench']['TPS'])
                if vm_result['pgbench']['Latency_Avg'] is not None:
                    pgbench_metrics['Latency_Avg'].append(vm_result['pgbench']['Latency_Avg'])
                pgbench_metrics['Transactions'].append(vm_result['pgbench']['Transactions'])
    
    if pgbench_found and pgbench_metrics['TPS']:
        samples = len(pgbench_metrics['TPS'])
        aggregated['pgbench'] = {
            'TPS_mean': mean(pgbench_metrics['TPS']),
            'TPS_stdev': stdev(pgbench_metrics['TPS']) if samples > 1 else 0,
            'Latency_Avg_mean': mean(pgbench_metrics['Latency_Avg']) if pgbench_metrics['Latency_Avg'] else 0,
            'Latency_Avg_stdev': stdev(pgbench_metrics['Latency_Avg']) if samples > 1 and pgbench_metrics['Latency_Avg'] else 0,
            'samples': samples
        }
    
    return aggregated

def debug_data_structure(aggregated_data, output_file):
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
    print("\nüîç –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  ‚Ä¢ –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {type(aggregated_data)}")
    
    if isinstance(aggregated_data, dict):
        print(f"  ‚Ä¢ –ö–ª—é—á–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è: {', '.join(aggregated_data.keys())}")
        
        if 'fio' in aggregated_data:
            print(f"  ‚Ä¢ –¢–µ—Å—Ç—ã FIO: {', '.join(aggregated_data['fio'].keys())}")
            for test_name, metrics in aggregated_data['fio'].items():
                print(f"    - {test_name}: {', '.join(metrics.keys())}")
        
        if 'pgbench' in aggregated_data:
            print(f"  ‚Ä¢ –î–∞–Ω–Ω—ã–µ pgbench: {', '.join(aggregated_data['pgbench'].keys())}")
        elif 'pgbench_section' in aggregated_data:
            print("  ‚Ä¢ –ù–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ pgbench –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    with open(f"{output_file}_structure_debug.txt", 'w') as f:
        json.dump(aggregated_data, f, indent=2, ensure_ascii=False)
    
    print(f"  ‚Ä¢ –ü–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_file}_structure_debug.txt")

def generate_report(aggregated, output_file):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    report = []
    report.append("="*80)
    report.append("–ê–ì–†–ï–ì–ò–†–û–í–ê–ù–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    report.append("="*80)
    report.append(f"–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {len(aggregated['iterations'])}")
    report.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –í–ú: {aggregated['num_vms']}")
    report.append("")
    report.append("‚ÑπÔ∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã")
    report.append("")
    
    # FIO —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if aggregated['fio']:
        report.append("="*80)
        report.append("FIO - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–æ–≤–æ–π –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã (—Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)")
        report.append("="*80)
        report.append("")
        report.append(f"{'Test Name':<35} {'IOPS_mean':<15} {'Bandwidth_mean (MiB/s)':<25} {'Latency_mean (ms)':<20} {'Samples':<8}")
        report.append("-"*80)
        for test_name, metrics in sorted(aggregated['fio'].items()):
            report.append(
                f"{test_name:<35} "
                f"{metrics['IOPS_mean']:>7.1f} ¬± {metrics['IOPS_stdev']:>4.1f}    "
                f"{metrics['Bandwidth_mean']:>7.1f} ¬± {metrics['Bandwidth_stdev']:>4.1f}                   "
                f"{metrics['Latency_mean']:>6.2f} ¬± {metrics['Latency_stdev']:>4.2f}           "
                f"{metrics['samples']:<8}"
            )
        report.append("")
    
    # pgbench —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if 'pgbench' in aggregated and aggregated['pgbench']:
        report.append("="*80)
        report.append("pgbench - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ PostgreSQL OLTP (—Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)")
        report.append("="*80)
        report.append("")
        pg = aggregated['pgbench']
        report.append(f"TPS (Transactions Per Second): {pg['TPS_mean']:.2f} ¬± {pg['TPS_stdev']:.2f}")
        report.append(f"–°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞: {pg['Latency_Avg_mean']:.3f} ¬± {pg['Latency_Avg_stdev']:.3f} ms")
        report.append(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π: {pg['samples']}")
        report.append("")
    
    report.append("="*80)
    report.append("–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ó–Ω–∞—á–µ–Ω–∏—è —É–∫–∞–∑–∞–Ω—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ '—Å—Ä–µ–¥–Ω–µ–µ ¬± —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ'")
    report.append("–ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º) –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã")
    report.append("="*80)
    
    report_text = "\n".join(report)
    
    # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
    print(report_text)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
    with open(output_file, 'w') as f:
        f.write(report_text)
    print(f"\nüìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
    return report_text

def save_json(aggregated, output_file):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ JSON —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
    with open(output_file, 'w') as f:
        json.dump(aggregated, f, indent=2, ensure_ascii=False)
    print(f"üìä JSON –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")

def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python3 aggregate_results.py <–ø—É—Ç—å_–∫_–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏_—Å_—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏> [–ø—É—Ç—å_2] ...")
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python3 aggregate_results.py results/20251218_1619_local_1vms_2iter/")
        print("  python3 aggregate_results.py results/*/")
        sys.exit(1)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    for results_dir in sys.argv[1:]:
        if not os.path.exists(results_dir):
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {results_dir}")
            continue
        
        print(f"\n{'='*60}")
        print(f"üìÅ –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤: {results_dir}")
        print("‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        aggregated = aggregate_results(results_dir)
        if not aggregated:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            continue
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö
        debug_data_structure(aggregated, os.path.join(results_dir, "aggregated_report"))
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
        output_base = os.path.join(results_dir, "aggregated_report")
        generate_report(aggregated, f"{output_base}.txt")
        save_json(aggregated, f"{output_base}.json")
    
    print(f"\n{'='*60}")
    print("‚úÖ –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –≤—Å–µ—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π!")

if __name__ == "__main__":
    main()